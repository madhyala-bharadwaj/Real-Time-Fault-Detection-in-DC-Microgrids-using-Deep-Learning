"""
Live Fault Inference Module

This script provides a clean, reusable `FaultDetector` class for real-time
inference. It is designed to be imported into a larger production application
(e.g., a monitoring service, a web API).

The class encapsulates the entire inference pipeline:
  1. Loads the pre-trained Keras model and data scalers.
  2. Processes raw voltage/current data streams into the required windowed format.
  3. Applies the correct scaling and feature extraction (wavelets).
  4. Returns predictions in a human-readable format.

---
Prerequisites:
- A trained model saved as a directory (e.g., 'transformer_model').
- A 'scalers.pkl' file containing the fitted StandardScaler objects.
Both artifacts are generated by `fault_detection_transformer_based.py`.

---
Example Usage:

from run_inference import FaultDetector
import numpy as np

# 1. Instantiate the detector (loads model and scalers)
try:
    detector = FaultDetector(
        model_path="transformer_model",
        scalers_path="scalers.pkl"
    )

    # 2. Simulate or acquire a live data stream
    # (In a real scenario, this data would come from sensors)
    live_voltage = np.random.randn(1000)
    live_current = np.random.randn(1000)

    # 3. Get predictions from the stream
    predictions = detector.predict_stream(live_voltage, live_current)

    # 4. Process the results
    for timestamp, label, confidence, inference_time in predictions:
        if label != "Normal":
            print(f"ALERT! {timestamp}: {label} (Conf: {confidence:.2%})")
        else:
            print(f"OK. {timestamp}: {label} (Conf: {confidence:.2%})")

except FileNotFoundError as e:
    print(e)
except Exception as e:
    print(f"An error occurred: {e}")
---
"""

import numpy as np
import tensorflow as tf
import pickle
import pywt
import time
import os
import logging


# Configure basic logging for the module
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
)


class FaultDetector:
    """
    Encapsulates a trained fault detection model for live inference.
    """

    def __init__(self, model_path: str, scalers_path: str):
        """
        Initializes the detector by loading the trained model and scalers.

        Args:
            model_path (str): Path to the saved Keras model directory.
            scalers_path (str): Path to the saved scalers .pkl file.

        Raises:
            FileNotFoundError: If the model or scalers file cannot be found.
        """
        logging.info("Initializing Fault Detector...")
        if not os.path.exists(model_path) or not os.path.exists(scalers_path):
            raise FileNotFoundError(
                f"Could not find required artifacts. Please ensure '{model_path}' "
                f"and '{scalers_path}' exist in the current directory."
            )

        # Load the compiled model. `compile=False` is often faster for inference.
        self.model = tf.keras.models.load_model(model_path, compile=False)
        logging.info(f"Model loaded successfully from '{model_path}'.")

        # Load the scaler objects.
        with open(scalers_path, "rb") as f:
            artifacts = pickle.load(f)
            self.scaler_voltage = artifacts["scaler_voltage"]
            self.scaler_current = artifacts["scaler_current"]
        logging.info(f"Scalers loaded successfully from '{scalers_path}'.")

        # Define model parameters from the loaded artifacts.
        self.window_size = self.model.input_shape[0][1]
        self.label_map = {0: "Normal", 1: "PP Fault", 2: "PPG Fault", 3: "NPG Fault"}
        self.wavelet_family = "db4"
        self.wavelet_level = 4

        logging.info(
            f"Initialization complete. Model expects windows of size {self.window_size}."
        )

    def _extract_wavelet_features(self, window: np.ndarray) -> np.ndarray:
        """
        Private helper to extract wavelet features from a single scaled window.

        Args:
            window (np.ndarray): A NumPy array of shape (window_size, 2).

        Returns:
            A 1D NumPy array of wavelet features.
        """
        features = []
        for j in range(window.shape[1]):  # Iterate through voltage and current
            coeffs = pywt.wavedec(
                window[:, j], self.wavelet_family, level=self.wavelet_level
            )
            for coef in coeffs:
                features.extend([np.mean(coef), np.std(coef), np.sum(np.square(coef))])
        return np.array(features)

    def _preprocess_window(self, window: np.ndarray) -> (np.ndarray, np.ndarray):
        """
        Private helper to apply the full preprocessing pipeline to a single window.

        Args:
            window (np.ndarray): The raw window of shape (window_size, 2).

        Returns:
            A tuple containing the processed (time_series_input, wavelet_input)
            ready for the model.
        """
        scaled_window = window.copy()

        # Apply the pre-fitted scalers.
        scaled_window[:, 0] = self.scaler_voltage.transform(
            scaled_window[:, 0].reshape(-1, 1)
        ).flatten()
        scaled_window[:, 1] = self.scaler_current.transform(
            scaled_window[:, 1].reshape(-1, 1)
        ).flatten()

        # Extract wavelet features from the scaled data.
        wavelet_features = self._extract_wavelet_features(scaled_window)

        # Reshape inputs to include the batch dimension (1, window_size, 2).
        ts_input = np.expand_dims(scaled_window, axis=0)
        wavelet_input = np.expand_dims(wavelet_features, axis=0)

        return ts_input, wavelet_input

    def predict_stream(
        self, voltage_stream: np.ndarray, current_stream: np.ndarray, stride: int = 10
    ):
        """
        Analyzes voltage and current streams using a sliding window and yields predictions.

        This is a generator function that processes the stream chunk by chunk.

        Args:
            voltage_stream (np.ndarray): A 1D NumPy array of voltage readings.
            current_stream (np.ndarray): A 1D NumPy array of current readings.
            stride (int): The number of time steps to slide the window forward
                          for each new prediction.

        Yields:
            A tuple containing:
            (timestamp_label, prediction_label, confidence, inference_time_ms)
        """
        if len(voltage_stream) != len(current_stream):
            raise ValueError(
                "Input voltage and current streams must have the same length."
            )

        # Combine the two 1D streams into a single 2D signal array.
        signal = np.stack([voltage_stream, current_stream], axis=-1)

        logging.info(
            f"Starting live prediction on input stream of length {len(signal)}..."
        )

        # Iterate through the signal using a sliding window.
        for start in range(0, len(signal) - self.window_size + 1, stride):
            end = start + self.window_size
            current_window = signal[start:end, :]

            # Apply the full preprocessing pipeline.
            ts_input, wavelet_input = self._preprocess_window(current_window)

            # Perform inference and time it.
            start_time = time.time()
            _, multiclass_pred, _ = self.model.predict(
                [ts_input, wavelet_input], verbose=0
            )
            inference_time_ms = (time.time() - start_time) * 1000

            # Interpret the model's output probabilities.
            predicted_class_index = np.argmax(multiclass_pred[0])
            confidence = np.max(multiclass_pred[0])
            prediction_label = self.label_map.get(predicted_class_index, "Unknown")

            timestamp_label = f"TimeStep_{start}-{end}"

            yield (timestamp_label, prediction_label, confidence, inference_time_ms)
